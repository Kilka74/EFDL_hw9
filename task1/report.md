# Task1 Report

In this directory I implemented all methods from the first task. In `model.py` I implemented ResNet model. If `pretrained=True` we have model with weights from pytorch. I also fixed first convolution for the training on $32 \times 32$ images and deleted max pool. In case of `pretrained=False` we will have untrained version of model with fixed `layer3`. In `train.py` I have all the training pipeline. `train` is basic train without any distillation, `train_distilled` is the variant with distillation. In my code I have flag `add_mse` in `train_distilled` which corresponds to whether we add mse term in loss or not. I set all the weights practically randomly, may be these parameters need to be tuned. But I tried several proportions and these proportions gave me best results. I logged everything in wandb, here is the [link](https://wandb.ai/kilka74/EFDL_hw9/table?nw=nwuserkilka74)

In this link I have 4 shown runs, every run corresponds to one subtask. We can see that with the growth of the complexity of the method I obtained better results. I set the number of epochs equal to 10 or 15 in all cases because I haven't got enough resources and usually it was enough for convergence. We can see that method with computing mse for activations performs better than others. We will use it in the next task. All runs I have in `hw9.ipynb`, but due to datasphere bug not all the runs are visible((( I couldn't fix it